# backend/services/smoke_flame_detection.py
"""
Smoke and Flame Detection Service â€”â€” çƒŸç«æ£€æµ‹æœåŠ¡
Two-stage detection pipeline:
1. YOLOv8 preliminary detection (smoke_flame.pt)
2. Qwen-VL API verification for positive cases
3. Double confirmation â†’ save event

åŒé˜¶æ®µæ£€æµ‹æµæ°´çº¿ï¼š
1. YOLOv8 åˆæ­¥æ£€æµ‹ï¼ˆsmoke_flame.ptï¼‰
2. é˜³æ€§ç»“æœ â†’ Qwen-VL API éªŒè¯
3. åŒé‡ç¡®è®¤ â†’ ä¿å­˜äº‹ä»¶
"""

import logging
import time
import base64
import requests
import threading
from typing import List, Dict, Any, Optional, Tuple
import cv2
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed

from ml_models.yolov8.inference import YOLOInference
from ml_models.yolov8.model_loader import YOLOModelLoader
from backend.services.event_generator import handle_frame_events
from backend.utils.visualization import render_official_frame
from storage.minio_client import MinIOClient
from storage.mongodb_client import MongoDBClient
from backend.utils.frame_capture import FrameWithMetadata

logger = logging.getLogger(__name__)


class SmokeFlameDetectionService:
    """çƒŸç«æ£€æµ‹æœåŠ¡ - åŒé˜¶æ®µéªŒè¯"""

    def __init__(self):
        self.minio_client = None
        self.mongo_client = None
        self.model_loader = None
        self.yolo_model = None
        self.qwen_vl_client = None
        self.frame_skip = 23  # è·³å¸§è®¾ç½®ï¼Œæ¯24å¸§å¤„ç†1å¸§ | Frame skipping, process 1 frame every 24 frames
        self.last_processed_frame = {}  # è®°å½•æ¯ä¸ªsource_idçš„æœ€åå¤„ç†å¸§ | Track last processed frame per source_id
        self.thread_pool = ThreadPoolExecutor(max_workers=6)  # å¢åŠ å¹¶è¡Œå·¥ä½œçº¿ç¨‹ | Increase parallel workers
        self.detection_cache = {}  # æ£€æµ‹ç»“æœç¼“å­˜ | Detection result cache



    # -------------------- ä¾èµ–æ³¨å…¥ --------------------
    def set_clients(self, minio_client: MinIOClient, mongo_client: MongoDBClient):
        """Set storage clients"""
        self.minio_client = minio_client
        self.mongo_client = mongo_client

    def set_model_loader(self, loader: YOLOModelLoader):
        """Set model loader"""
        self.model_loader = loader
        self.yolo_model = self.model_loader.get_model("smoke_flame")

    def set_qwen_vl_client(self, qwen_client):
        """Set Qwen-VL client"""
        self.qwen_vl_client = qwen_client

    def set_frame_skip(self, skip_frames: int):
        """Set frame skipping"""
        self.frame_skip = skip_frames
        logger.info(f"ğŸ”„ Set frame skip to {skip_frames} frames")

    # -------------------- ä¸»æ£€æµ‹æµç¨‹ --------------------
    def process_frame(self, frame_meta: FrameWithMetadata) -> None:
        """
        å¤„ç†å•å¸§çš„çƒŸç«æ£€æµ‹æµç¨‹
        Process single frame for smoke/flame detection pipeline - optimized version

        Args:
            frame_meta: å¸§å…ƒæ•°æ® | Frame metadata
        """
        if not all([self.minio_client, self.mongo_client, self.yolo_model]):
            logger.error("âŒ Service not fully initialized.")
            return

        source_id = frame_meta.source_id
        frame_index = frame_meta.frame_index

        # 1. è·³å¸§æ£€æµ‹ - å¤§å¹…å‡å°‘å¤„ç†å¸§æ•° | Frame skipping - significantly reduce processed frames
        if not self._should_process_frame(source_id, frame_index):
            return

        # 2. æ›´æ–°æœ€åå¤„ç†å¸§ | Update last processed frame
        self.last_processed_frame[source_id] = frame_index

        image = frame_meta.image

        # ä½¿ç”¨å½“å‰ç³»ç»Ÿæ—¶é—´æˆ³ | Use current system timestamp
        current_timestamp = time.time()

        logger.debug(f"ğŸ”¥ Processing frame {frame_index} for {source_id}")

        # 3. å¼‚æ­¥å¤„ç†æ£€æµ‹æµç¨‹ | Async process detection pipeline
        future = self.thread_pool.submit(self._process_frame_async, frame_meta, current_timestamp)
        future.add_done_callback(lambda f: self._handle_processing_result(f, source_id, frame_index))

    def _should_process_frame(self, source_id: str, frame_index: int) -> bool:
        """
        Determine if current frame should be processed.
        """
        if source_id not in self.last_processed_frame:
            self.last_processed_frame[source_id] = -1
            return True

        last_processed = self.last_processed_frame[source_id]
        frames_since_last = frame_index - last_processed

        return frames_since_last > self.frame_skip

    def _process_frame_async(self, frame_meta: FrameWithMetadata, timestamp: float) -> Dict[str, Any]:
        """
        Async frame processing.
        """
        try:
            source_id = frame_meta.source_id
            image = frame_meta.image

            # 1. YOLOv8 åˆæ­¥æ£€æµ‹ | YOLOv8 preliminary detection
            yolo_detections = self._yolo_detection_enhanced(image)

            if not yolo_detections:
                return {"source_id": source_id, "detections": [], "timestamp": timestamp, "frame_meta": frame_meta}

            logger.info(f"ğŸ”¥ YOLOv8 detected {len(yolo_detections)} potential smoke/flame regions in {source_id}")

            # 2. å¯¹æ£€æµ‹ç»“æœè¿›è¡Œåˆ†ç»„å’Œç­›é€‰ | Group and filter detection results
            filtered_detections = self._filter_and_group_detections(yolo_detections, image.shape)

            if not filtered_detections:
                logger.debug(f"ğŸŸ¡ No valid detections after filtering in {source_id}")
                return {"source_id": source_id, "detections": [], "timestamp": timestamp, "frame_meta": frame_meta}

            logger.info(f"ğŸ” After filtering: {len(filtered_detections)} regions for Qwen-VL verification")

            # 3. Qwen-VL API éªŒè¯ - å¹¶è¡Œå¤„ç†å¤šä¸ªæ£€æµ‹åŒºåŸŸ | Qwen-VL API verification - parallel processing
            verified_detections = self._qwen_vl_verification_parallel(image, filtered_detections, source_id)

            if not verified_detections:
                logger.debug(f"ğŸŸ¡ Qwen-VL rejected all YOLOv8 detections in {source_id}")
                return {"source_id": source_id, "detections": [], "timestamp": timestamp, "frame_meta": frame_meta}

            logger.info(f"âœ… Qwen-VL verified {len(verified_detections)} smoke/flame events in {source_id}")

            return {
                "source_id": source_id,
                "detections": verified_detections,
                "timestamp": timestamp,
                "frame_meta": frame_meta
            }

        except Exception as e:
            logger.error(f"âŒ Async frame processing failed: {e}")
            return {"source_id": frame_meta.source_id, "detections": [], "timestamp": timestamp,
                    "frame_meta": frame_meta}

    def _handle_processing_result(self, future, source_id: str, frame_index: int):
        """
        å¤„ç†å¼‚æ­¥æ£€æµ‹ç»“æœ | Handle async detection results
        """
        try:
            result = future.result()
            detections = result["detections"]

            if detections:
                # ä¿å­˜æ£€æµ‹äº‹ä»¶ | Save detection events
                self._save_detection_events(result["frame_meta"], detections, result["timestamp"])

        except Exception as e:
            logger.error(f"âŒ Error handling processing result for {source_id} frame {frame_index}: {e}")

    def _yolo_detection_enhanced(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """
        YOLOv8 çƒŸç«åˆæ­¥æ£€æµ‹ - å¢å¼ºç‰ˆæœ¬ï¼Œæé«˜æ£€æµ‹çµæ•åº¦
        YOLOv8 preliminary smoke/flame detection - enhanced version with higher sensitivity

        Args:
            image: è¾“å…¥å›¾åƒ | Input image

        Returns:
            List[Dict]: æ£€æµ‹ç»“æœåˆ—è¡¨ | List of detection results
        """
        try:
            # ä¿æŒåŸå§‹åˆ†è¾¨ç‡ä»¥æé«˜å°ç›®æ ‡æ£€æµ‹èƒ½åŠ› | Keep original resolution for better small object detection
            original_shape = image.shape

            # åªåœ¨å›¾åƒå¾ˆå¤§æ—¶æ‰é™é‡‡æ · | Only downsample if image is very large
            if original_shape[0] > 1080 or original_shape[1] > 1920:
                scale_factor = 1080 / original_shape[0]
                new_width = int(original_shape[1] * scale_factor)
                resized_image = cv2.resize(image, (new_width, 1080))
                logger.debug(f"ğŸ–¼ï¸ Resized image from {original_shape} to {resized_image.shape}")
            else:
                resized_image = image

            # è¿è¡ŒYOLOv8æ£€æµ‹ - ä½¿ç”¨æ›´ä½çš„ç½®ä¿¡åº¦é˜ˆå€¼ | Run YOLOv8 detection - use lower confidence threshold
            start_time = time.time()
            raw_detections = YOLOInference.run_detection(
                self.yolo_model, resized_image, conf_threshold=0.10  # é™ä½é˜ˆå€¼æé«˜å¬å›ç‡ | Lower threshold for better recall
            )
            detection_time = time.time() - start_time
            logger.debug(f"â±ï¸ YOLOv8 detection time: {detection_time:.3f}s")

            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ | Convert to standard format
            detections = []
            for cls, conf, bbox in raw_detections:
                if cls.lower() in ["smoke", "fire", "flame"]:
                    # å¦‚æœè°ƒæ•´äº†å›¾åƒå¤§å°ï¼Œéœ€è¦è°ƒæ•´è¾¹ç•Œæ¡†åæ ‡ | If image was resized, adjust bbox coordinates
                    if resized_image is not image:
                        scale_x = original_shape[1] / resized_image.shape[1]
                        scale_y = original_shape[0] / resized_image.shape[0]
                        x1, y1, x2, y2 = bbox
                        bbox = (x1 * scale_x, y1 * scale_y, x2 * scale_x, y2 * scale_y)

                    # ç¡®ä¿è¾¹ç•Œæ¡†åæ ‡æ˜¯æ•´æ•° | Ensure bbox coordinates are integers
                    x1, y1, x2, y2 = map(int, bbox)
                    normalized_bbox = (x1, y1, x2, y2)

                    # è®¡ç®—è¾¹ç•Œæ¡†é¢ç§¯ | Calculate bbox area
                    bbox_area = (x2 - x1) * (y2 - y1)
                    image_area = original_shape[0] * original_shape[1]
                    area_ratio = bbox_area / image_area

                    # è¿‡æ»¤æ‰å¤ªå°çš„æ£€æµ‹æ¡† | Filter out too small detections
                    if area_ratio < 0.0001:  # å°äºå›¾åƒé¢ç§¯çš„0.01%
                        logger.debug(f"ğŸ” Skipped small detection: {area_ratio:.6f}")
                        continue

                    detections.append({
                        "class_name": cls,
                        "confidence": float(conf),
                        "bbox": normalized_bbox,
                        "detection_stage": "yolo_initial",
                        "area_ratio": area_ratio
                    })

            # æŒ‰ç½®ä¿¡åº¦æ’åº | Sort by confidence
            detections.sort(key=lambda x: x["confidence"], reverse=True)

            logger.debug(f"ğŸ”¥ YOLOv8 raw detections: {len(raw_detections)}, filtered: {len(detections)}")
            return detections

        except Exception as e:
            logger.error(f"âŒ YOLOv8 detection failed: {e}")
            return []

    def _filter_and_group_detections(self, detections: List[Dict[str, Any]], image_shape: Tuple[int, int, int]) -> List[
        Dict[str, Any]]:
        """
        è¿‡æ»¤å’Œåˆ†ç»„æ£€æµ‹ç»“æœï¼Œé¿å…é‡å¤æ£€æµ‹åŒä¸€åŒºåŸŸ
        Filter and group detection results to avoid duplicate detections in same area

        Args:
            detections: åŸå§‹æ£€æµ‹ç»“æœ | Original detection results
            image_shape: å›¾åƒå½¢çŠ¶ | Image shape

        Returns:
            List[Dict]: è¿‡æ»¤åçš„æ£€æµ‹ç»“æœ | Filtered detection results
        """
        if not detections:
            return []

        h, w = image_shape[:2]
        filtered_detections = []
        used_areas = []  # è®°å½•å·²ä½¿ç”¨çš„åŒºåŸŸ | Record used areas

        for detection in detections:
            try:
                bbox = detection["bbox"]
                x1, y1, x2, y2 = bbox

                # è®¡ç®—å½“å‰æ£€æµ‹æ¡†çš„ä¸­å¿ƒç‚¹ | Calculate center point of current bbox
                center_x = (x1 + x2) / 2
                center_y = (y1 + y2) / 2

                # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰æ£€æµ‹æ¡†é‡å  | Check if overlaps with existing detections
                is_duplicate = False
                for used_bbox in used_areas:
                    ux1, uy1, ux2, uy2 = used_bbox
                    # è®¡ç®—IoU | Calculate IoU
                    inter_x1 = max(x1, ux1)
                    inter_y1 = max(y1, uy1)
                    inter_x2 = min(x2, ux2)
                    inter_y2 = min(y2, uy2)

                    if inter_x1 < inter_x2 and inter_y1 < inter_y2:
                        # è®¡ç®—é‡å é¢ç§¯ | Calculate overlap area
                        inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
                        current_area = (x2 - x1) * (y2 - y1)
                        used_area = (ux2 - ux1) * (uy2 - uy1)

                        iou = inter_area / min(current_area, used_area)

                        # å¦‚æœIoUå¤§äºé˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯é‡å¤æ£€æµ‹ | If IoU > threshold, consider as duplicate
                        if iou > 0.3:
                            is_duplicate = True
                            logger.debug(f"ğŸ” Skipped duplicate detection with IoU: {iou:.3f}")
                            break

                if not is_duplicate:
                    filtered_detections.append(detection)
                    used_areas.append(bbox)

            except Exception as e:
                logger.error(f"âŒ Error in detection filtering: {e}")
                continue

        logger.debug(f"ğŸ” Detection filtering: {len(detections)} -> {len(filtered_detections)}")
        return filtered_detections

    def _qwen_vl_verification_parallel(self, image: np.ndarray, yolo_detections: List[Dict[str, Any]],
                                       source_id: str) -> List[Dict[str, Any]]:
        """
        Qwen-VL API åŒé‡éªŒè¯ - å¹¶è¡Œç‰ˆæœ¬
        Qwen-VL API double verification - parallel version

        Args:
            image: åŸå§‹å›¾åƒ | Original image
            yolo_detections: YOLOv8æ£€æµ‹ç»“æœ | YOLOv8 detection results
            source_id: æºæ ‡è¯† | Source identifier

        Returns:
            List[Dict]: éªŒè¯é€šè¿‡çš„æ£€æµ‹ç»“æœ | Verified detection results
        """
        if not self.qwen_vl_client:
            logger.warning("âš ï¸ Qwen-VL client not available, using YOLOv8 results directly")
            return yolo_detections

        # å¢åŠ æœ€å¤§å¹¶è¡ŒéªŒè¯æ•°é‡ | Increase maximum parallel verifications
        max_parallel_verifications = 8  # å¢åŠ åˆ°8ä¸ªå¹¶è¡ŒéªŒè¯
        detections_to_verify = yolo_detections[:max_parallel_verifications]

        verified_detections = []
        verification_futures = {}

        # æäº¤å¹¶è¡ŒéªŒè¯ä»»åŠ¡ | Submit parallel verification tasks
        for i, detection in enumerate(detections_to_verify):
            try:
                # æå–æ£€æµ‹åŒºåŸŸ | Extract detection region
                bbox = detection["bbox"]
                cropped_image = self._crop_detection_region(image, bbox)

                if cropped_image is None:
                    continue

                # æäº¤éªŒè¯ä»»åŠ¡ | Submit verification task
                future = self.thread_pool.submit(
                    self._single_verification,
                    cropped_image, detection, i, source_id
                )
                verification_futures[future] = (detection, i)

            except Exception as e:
                logger.error(f"âŒ Failed to submit verification for detection {i + 1}: {e}")
                # éªŒè¯å¤±è´¥æ—¶ä¿å®ˆå¤„ç†ï¼šä¿ç•™YOLOç»“æœ | Conservative approach: keep YOLO result on failure
                verified_detections.append(detection)

        # æ”¶é›†éªŒè¯ç»“æœ | Collect verification results
        for future in as_completed(verification_futures):
            detection, original_index = verification_futures[future]
            try:
                is_verified = future.result()
                if is_verified:
                    verified_detection = detection.copy()
                    verified_detection["confidence"] = (detection["confidence"] + 0.8) / 2
                    verified_detection["detection_stage"] = "qwen_verified"
                    verified_detections.append(verified_detection)
                    logger.info(f"âœ… Qwen-VL verified detection {original_index + 1}: {detection['class_name']}")
                else:
                    logger.info(f"âŒ Qwen-VL rejected detection {original_index + 1}: {detection['class_name']}")
            except Exception as e:
                logger.error(f"âŒ Verification failed for detection {original_index + 1}: {e}")
                # éªŒè¯å¤±è´¥æ—¶ä¿å®ˆå¤„ç†ï¼šä¿ç•™YOLOç»“æœ
                verified_detections.append(detection)

        return verified_detections

    def _single_verification(self, cropped_image: np.ndarray, detection: Dict[str, Any],
                             index: int, source_id: str) -> bool:
        """
        å•ä¸ªæ£€æµ‹åŒºåŸŸçš„éªŒè¯ | Single detection region verification
        """
        try:
            start_time = time.time()
            is_verified = self.qwen_vl_client.verify_smoke_flame(cropped_image)
            verification_time = time.time() - start_time
            logger.debug(f"â±ï¸ Qwen-VL verification {index + 1} time: {verification_time:.3f}s")
            return is_verified
        except Exception as e:
            logger.error(f"âŒ Single verification failed for detection {index + 1}: {e}")
            return False

    def _crop_detection_region(self, image: np.ndarray, bbox: Tuple[int, int, int, int]) -> Optional[np.ndarray]:
        """
        è£å‰ªæ£€æµ‹åŒºåŸŸç”¨äºQwen-VLåˆ†æ - å¢åŠ è¾¹ç•Œæ‰©å±•
        Crop detection region for Qwen-VL analysis - with border expansion

        Args:
            image: åŸå§‹å›¾åƒ | Original image
            bbox: è¾¹ç•Œæ¡† (x1, y1, x2, y2) | Bounding box (x1, y1, x2, y2)

        Returns:
            Optional[np.ndarray]: è£å‰ªåçš„å›¾åƒ | Cropped image
        """
        try:
            x1, y1, x2, y2 = bbox

            # æ‰©å±•è¾¹ç•Œæ¡†ä»¥åŒ…å«æ›´å¤šä¸Šä¸‹æ–‡ | Expand bbox to include more context
            expand_ratio = 0.2  # æ‰©å±•20%
            width = x2 - x1
            height = y2 - y1

            x1_expanded = max(0, int(x1 - width * expand_ratio))
            y1_expanded = max(0, int(y1 - height * expand_ratio))
            x2_expanded = min(image.shape[1], int(x2 + width * expand_ratio))
            y2_expanded = min(image.shape[0], int(y2 + height * expand_ratio))

            # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†… | Ensure coordinates are within image bounds
            h, w = image.shape[:2]
            x1_expanded = max(0, x1_expanded)
            y1_expanded = max(0, y1_expanded)
            x2_expanded = min(w, x2_expanded)
            y2_expanded = min(h, y2_expanded)

            if x2_expanded <= x1_expanded or y2_expanded <= y1_expanded:
                return None

            cropped = image[y1_expanded:y2_expanded, x1_expanded:x2_expanded]

            # ç¡®ä¿è£å‰ªåŒºåŸŸè¶³å¤Ÿå¤§ | Ensure cropped region is large enough
            if cropped.size == 0 or cropped.shape[0] < 10 or cropped.shape[1] < 10:
                return None

            # é™åˆ¶æœ€å¤§å°ºå¯¸ä»¥å‡å°‘APIä¼ è¾“æ—¶é—´ | Limit maximum size to reduce API transmission time
            max_size = 512
            if cropped.shape[0] > max_size or cropped.shape[1] > max_size:
                scale = max_size / max(cropped.shape[0], cropped.shape[1])
                new_width = int(cropped.shape[1] * scale)
                new_height = int(cropped.shape[0] * scale)
                cropped = cv2.resize(cropped, (new_width, new_height))

            return cropped

        except Exception as e:
            logger.error(f"âŒ Failed to crop detection region: {e}")
            return None

    def _save_detection_events(self, frame_meta: FrameWithMetadata,
                               detections: List[Dict[str, Any]], timestamp: float):
        """
        ä¿å­˜æ£€æµ‹äº‹ä»¶åˆ°å­˜å‚¨
        Save detection events to storage

        Args:
            frame_meta: å¸§å…ƒæ•°æ® | Frame metadata
            detections: æ£€æµ‹ç»“æœ | Detection results
            timestamp: æ—¶é—´æˆ³ | Timestamp
        """
        try:
            source_id = frame_meta.source_id
            image = frame_meta.image

            # ç¡®ä¿æ£€æµ‹ç»“æœçš„è¾¹ç•Œæ¡†æ ¼å¼æ­£ç¡® | Ensure bbox format is correct in detection results
            validated_detections = self._validate_detection_bboxes(detections, image.shape)

            # æ¸²æŸ“ç»“æœå›¾åƒ | Render result image
            rendered = render_official_frame(
                image=image,
                all_detections=validated_detections,
                violations=validated_detections,
                zones=None
            )

            # ä¸Šä¼ åˆ°MinIO | Upload to MinIO
            image_url = self.minio_client.upload_frame(
                image_data=rendered,
                camera_id=source_id,
                timestamp=timestamp,
                event_type="smoke_flame"
            )

            if not image_url:
                logger.error("âŒ Failed to upload smoke/flame detection image")
                return

            # æ¸…ç†URL | Clean URL
            clean_url = image_url.split('?')[0] if '?X-Amz-' in image_url else image_url
            logger.info(f"ğŸ“¸ Smoke/Flame image URL: {clean_url}")

            # ä¿å­˜åˆ°MongoDB | Save to MongoDB
            ok = handle_frame_events(
                minio_client=self.minio_client,
                mongo_client=self.mongo_client,
                image_url=clean_url,
                camera_id=source_id,
                timestamp=timestamp,
                frame_index=frame_meta.frame_index,
                violations=validated_detections
            )

            if ok:
                logger.info(f"âœ… Smoke/Flame events saved: {len(validated_detections)} detections")
            else:
                logger.error("âŒ Failed to save smoke/flame events to database")

        except Exception as e:
            logger.error(f"âŒ Failed to save smoke/flame detection events: {e}")

    def _validate_detection_bboxes(self, detections: List[Dict[str, Any]], image_shape: Tuple[int, int, int]) -> List[
        Dict[str, Any]]:
        """
        éªŒè¯å’Œä¿®å¤æ£€æµ‹ç»“æœçš„è¾¹ç•Œæ¡†æ ¼å¼
        Validate and fix bbox format in detection results

        Args:
            detections: æ£€æµ‹ç»“æœåˆ—è¡¨ | List of detection results
            image_shape: å›¾åƒå½¢çŠ¶ (h, w, c) | Image shape (h, w, c)

        Returns:
            List[Dict]: éªŒè¯åçš„æ£€æµ‹ç»“æœ | Validated detection results
        """
        validated_detections = []
        h, w = image_shape[:2]

        for detection in detections:
            try:
                bbox = detection["bbox"]

                # æ£€æŸ¥è¾¹ç•Œæ¡†æ ¼å¼ | Check bbox format
                if isinstance(bbox, (list, tuple)) and len(bbox) == 4:
                    # ç¡®ä¿æ‰€æœ‰åæ ‡éƒ½æ˜¯æ•´æ•° | Ensure all coordinates are integers
                    x1, y1, x2, y2 = map(int, bbox)

                    # ç¡®ä¿åæ ‡åœ¨åˆç†èŒƒå›´å†… | Ensure coordinates are within reasonable range
                    x1 = max(0, min(x1, w))
                    y1 = max(0, min(y1, h))
                    x2 = max(0, min(x2, w))
                    y2 = max(0, min(y2, h))

                    # ç¡®ä¿è¾¹ç•Œæ¡†æœ‰æ•ˆ | Ensure bbox is valid
                    if x2 > x1 and y2 > y1:
                        validated_detection = detection.copy()
                        validated_detection["bbox"] = (x1, y1, x2, y2)
                        validated_detections.append(validated_detection)
                    else:
                        logger.warning(f"âš ï¸ Invalid bbox skipped: {bbox}")
                else:
                    logger.warning(f"âš ï¸ Invalid bbox format: {bbox}, type: {type(bbox)}")

            except Exception as e:
                logger.error(f"âŒ Error validating bbox {detection.get('bbox')}: {e}")
                # è·³è¿‡æ— æ•ˆçš„æ£€æµ‹ | Skip invalid detection

        if len(validated_detections) != len(detections):
            logger.warning(f"âš ï¸ Bbox validation: {len(detections)} -> {len(validated_detections)} valid detections")

        return validated_detections

    def flush_remaining(self):
        """å¤„ç†å‰©ä½™å¸§ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰| Process remaining frames (compatibility method)"""
        logger.info("ğŸ”„ Smoke/Flame detection - Flush remaining called")
        # ç­‰å¾…æ‰€æœ‰å¼‚æ­¥ä»»åŠ¡å®Œæˆ | Wait for all async tasks to complete
        self.thread_pool.shutdown(wait=True)


# -------------------- Qwen-VL API å®¢æˆ·ç«¯ --------------------
class QwenVLAPIClient:
    """Qwen-VL API å®¢æˆ·ç«¯ | Qwen-VL API Client"""

    def __init__(self, api_url: str, api_key: str, model_name: str = "qwen-vl-plus"):
        """
        åˆå§‹åŒ–Qwen-VL APIå®¢æˆ·ç«¯
        Initialize Qwen-VL API client

        Args:
            api_url: APIç«¯ç‚¹ | API endpoint (e.g., DashScope, OpenAI-compatible API)
            api_key: APIå¯†é’¥ | API key
            model_name: æ¨¡å‹åç§° | Model name
        """
        self.api_url = api_url
        self.api_key = api_key
        self.model_name = model_name
        self.verify_prompt = "è¯·ä»”ç»†åˆ†æè¿™å¼ å›¾ç‰‡ä¸­æ˜¯å¦æœ‰çƒŸé›¾æˆ–ç«ç„°ã€‚åªå›ç­”'æ˜¯'æˆ–'å¦'ï¼Œä¸è¦è§£é‡Šã€‚"
        self.timeout = 20  # ç¨å¾®å¢åŠ è¶…æ—¶æ—¶é—´ | Slightly increase timeout

    def verify_smoke_flame(self, image: np.ndarray) -> bool:
        """
        éªŒè¯å›¾åƒä¸­æ˜¯å¦æœ‰çƒŸé›¾æˆ–ç«ç„°
        Verify if there is smoke or flame in the image

        Args:
            image: è¾“å…¥å›¾åƒ | Input image

        Returns:
            bool: æ˜¯å¦å­˜åœ¨çƒŸé›¾æˆ–ç«ç„° | Whether smoke or flame exists
        """
        try:
            # ç¼–ç å›¾åƒä¸ºbase64 | Encode image to base64
            success, encoded_image = cv2.imencode('.jpg', image, [cv2.IMWRITE_JPEG_QUALITY, 85])  # é€‚å½“æé«˜è´¨é‡
            if not success:
                logger.error("âŒ Failed to encode image for Qwen-VL")
                return False

            image_base64 = base64.b64encode(encoded_image).decode('utf-8')

            # æ„é€ è¯·æ±‚è½½è· | Construct request payload
            payload = self._build_request_payload(image_base64)

            # å‘é€è¯·æ±‚ | Send request
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }

            logger.debug(f"ğŸ” Sending request to Qwen-VL API: {self.api_url}")
            start_time = time.time()
            response = requests.post(self.api_url, json=payload, headers=headers, timeout=self.timeout)
            response_time = time.time() - start_time
            logger.debug(f"â±ï¸ Qwen-VL API response time: {response_time:.3f}s")

            response.raise_for_status()

            # è§£æå“åº” | Parse response
            result = response.json()
            logger.debug(f"ğŸ” Qwen-VL API raw response: {result}")

            # è§£æå›ç­” | Parse answer
            answer = self._parse_response(result)
            logger.debug(f"ğŸ” Qwen-VL parsed answer: '{answer}'")

            return self._parse_verification_result(answer)

        except requests.exceptions.Timeout:
            logger.error("âŒ Qwen-VL API request timeout")
            return False
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Qwen-VL API request failed: {e}")
            return False
        except Exception as e:
            logger.error(f"âŒ Qwen-VL verification failed: {e}")
            return False

    def _build_request_payload(self, image_base64: str) -> Dict[str, Any]:
        """
        æ„å»ºAPIè¯·æ±‚è½½è·
        Build API request payload

        Args:
            image_base64: base64ç¼–ç çš„å›¾åƒ | Base64 encoded image

        Returns:
            Dict: è¯·æ±‚è½½è· | Request payload
        """
        # æ–¹æ³•1: OpenAIå…¼å®¹æ ¼å¼ï¼ˆé€‚ç”¨äºå¤§å¤šæ•°VLæ¨¡å‹APIï¼‰
        if "openai" in self.api_url or "v1/chat/completions" in self.api_url:
            return {
                "model": self.model_name,
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_base64}"
                                }
                            },
                            {
                                "type": "text",
                                "text": self.verify_prompt
                            }
                        ]
                    }
                ],
                "max_tokens": 10,
                "temperature": 0.1
            }
        # æ–¹æ³•2: DashScopeæ ¼å¼ï¼ˆé˜¿é‡Œäº‘é€šä¹‰åƒé—®ï¼‰
        elif "dashscope" in self.api_url:
            return {
                "model": self.model_name,
                "input": {
                    "messages": [
                        {
                            "role": "user",
                            "content": [
                                {
                                    "image": f"data:image/jpeg;base64,{image_base64}"
                                },
                                {
                                    "text": self.verify_prompt
                                }
                            ]
                        }
                    ]
                },
                "parameters": {
                    "max_tokens": 10,
                    "temperature": 0.1
                }
            }
        # æ–¹æ³•3: é€šç”¨æ ¼å¼
        else:
            return {
                "model": self.model_name,
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "image": f"data:image/jpeg;base64,{image_base64}"
                            },
                            {
                                "type": "text",
                                "text": self.verify_prompt
                            }
                        ]
                    }
                ],
                "max_tokens": 10,
                "temperature": 0.1
            }

    def _parse_response(self, response_data: Dict[str, Any]) -> str:
        """
        è§£æAPIå“åº” - ä¿®å¤åˆ—è¡¨å¯¹è±¡é”™è¯¯
        Parse API response - fix 'List' object has no attribute 'strip' error

        Args:
            response_data: APIå“åº”æ•°æ® | API response data

        Returns:
            str: æ¨¡å‹å›ç­”æ–‡æœ¬ | Model answer text
        """
        try:
            logger.debug(f"ğŸ” Raw response type: {type(response_data)}")
            logger.debug(
                f"ğŸ” Raw response keys: {response_data.keys() if isinstance(response_data, dict) else 'Not a dict'}")

            # å¤„ç†åˆ—è¡¨å“åº”çš„æƒ…å†µ
            if isinstance(response_data, list):
                logger.debug(f"ğŸ” Response is a list, length: {len(response_data)}")
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå°è¯•æå–ç¬¬ä¸€ä¸ªå…ƒç´ çš„æ–‡æœ¬å†…å®¹
                if response_data:
                    first_item = response_data[0]
                    if isinstance(first_item, dict):
                        # å°è¯•ä»å­—å…¸ä¸­æå–æ–‡æœ¬å†…å®¹
                        for content_key in ['content', 'text', 'message', 'result']:
                            if content_key in first_item:
                                content = first_item[content_key]
                                if isinstance(content, str):
                                    return content.strip().lower()
                                elif isinstance(content, list):
                                    # å¦‚æœå†…å®¹è¿˜æ˜¯åˆ—è¡¨ï¼Œç»§ç»­é€’å½’å¤„ç†
                                    return self._parse_response(content)
                    # å¦‚æœæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œåˆå¹¶æ‰€æœ‰å­—ç¬¦ä¸²
                    elif isinstance(first_item, str):
                        return " ".join([str(item).strip() for item in response_data]).lower()
                return ""

            # OpenAIå…¼å®¹æ ¼å¼
            if "choices" in response_data:
                choice = response_data["choices"][0]
                if "message" in choice and "content" in choice["message"]:
                    content = choice["message"]["content"]
                    if isinstance(content, str):
                        return content.strip().lower()
                    elif isinstance(content, list):
                        # å¦‚æœcontentæ˜¯åˆ—è¡¨ï¼Œé€’å½’å¤„ç†
                        return self._parse_response(content)

            # DashScopeæ ¼å¼
            elif "output" in response_data and "choices" in response_data["output"]:
                choice = response_data["output"]["choices"][0]
                if "message" in choice and "content" in choice["message"]:
                    content = choice["message"]["content"]
                    if isinstance(content, str):
                        return content.strip().lower()
                    elif isinstance(content, list):
                        return self._parse_response(content)

            # å°è¯•é€šç”¨è§£æ
            for key in ["content", "text", "result", "message"]:
                if key in response_data:
                    content = response_data[key]
                    if isinstance(content, str):
                        return content.strip().lower()
                    elif isinstance(content, list):
                        return self._parse_response(content)

            # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›åŸå§‹å“åº”çš„å­—ç¬¦ä¸²è¡¨ç¤ºç”¨äºè°ƒè¯•
            logger.warning(f"âš ï¸ Unrecognized API response format: {response_data}")
            return str(response_data)

        except Exception as e:
            logger.error(f"âŒ Failed to parse Qwen-VL response: {e}")
            logger.error(f"ğŸ” Problematic response data: {response_data}")
            return ""

    def _parse_verification_result(self, answer: str) -> bool:
        """
        è§£æéªŒè¯ç»“æœ - å¢å¼ºå®¹é”™æ€§
        Parse verification result - enhanced error tolerance

        Args:
            answer: æ¨¡å‹å›ç­” | Model answer

        Returns:
            bool: éªŒè¯ç»“æœ | Verification result
        """
        if not answer:
            logger.warning("âš ï¸ Empty Qwen-VL answer, treating as negative")
            return False

        # æ¸…ç†å›ç­”æ–‡æœ¬
        cleaned_answer = answer.strip().lower()

        # ä¸­æ–‡è‚¯å®šå›ç­” | Chinese affirmative answers
        chinese_yes = any(
            word in cleaned_answer for word in ["æ˜¯", "æœ‰", "å­˜åœ¨", "ç¡®è®¤", "yes", "true", "å¯¹çš„", "æ­£ç¡®"])
        # ä¸­æ–‡å¦å®šå›ç­” | Chinese negative answers
        chinese_no = any(
            word in cleaned_answer for word in ["å¦", "æ²¡æœ‰", "ä¸å­˜åœ¨", "æœªå‘ç°", "no", "false", "ä¸æ˜¯", "é”™è¯¯"])

        if chinese_yes and not chinese_no:
            logger.info(f"âœ… Qwen-VL confirmed: '{cleaned_answer}'")
            return True
        elif chinese_no and not chinese_yes:
            logger.info(f"âŒ Qwen-VL rejected: '{cleaned_answer}'")
            return False
        else:
            # æ¨¡ç³Šå›ç­”ï¼Œä¿å®ˆå¤„ç†ä¸ºå¦å®š | Ambiguous answer, conservatively treat as negative
            logger.warning(f"âš ï¸ Ambiguous Qwen-VL answer: '{cleaned_answer}', treating as negative")
            return False


# -------------------- å…¨å±€å®ä¾‹ --------------------
smoke_flame_detection_service = SmokeFlameDetectionService()